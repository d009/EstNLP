{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h1 style=\"color:blue\">Praktikum 11. </h1>\n",
    "<h3 style=\"color:blue\">Twitter'i säutsude analüüs. Info eraldamine. Suurte andmetega töötamine. Vikipeedia artiklite töötlus. Grammatikad</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 17. november 2017\n",
    "#### Ülesannete esitamise tähtaeg 26. november 2017 23:55"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tänases praktikumis tegeleme Twitteri säutsude ja Vikipeedia artiklite analüüsiga ning katsetame automaatset info eraldamist. Viimasega oleme vähesel määral kokku puutunud ka varasemates praktikumides - näiteks leides ajaväljendeid, nimega üksusi, sagedasemaid nimisõnafraase, elusolendeid erinevatest tekstidest jms. Siinses praktikumis käsitleme teemat aga süstemaatilisemalt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Info eraldamine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Informatsiooni saab eraldada nii struktureeritud andmetest kui ka vabast tekstist. Lähtudes käesoleva aine eesmärkidest, tegeleme siin just nimelt tekstiliste andmetega. Vabast (või poolstruktureeritud) tekstist info eraldamisel on eesmärgiks saada sealt kätte info niisugusel moel, et seda oleks võimalik esitada struktureeritud kujul - näiteks tabelina. Leitud infot kasutavad sõltuvalt eesmärkidest kas lõppkasutajad või teised süsteemid (otsingumootorid, andmebaasisüsteemid vms). \n",
    "\n",
    "Vabast tekstist info eraldamisega tehti algust juba 1970ndatel aastatel, laiemat kõlapinda on see leidnud aga alates 1990ndatest aastatest. Koos aina kasvavate (struktureerimata) infohulkadega muutub ka ülesanne aina päevakorralisemaks. \n",
    "\n",
    "Info eraldamise põhiülesanneteks ongi näiteks nimeolemite tuvastamine, samaviiteliste üksuste tuvastamine ja relatsioonide leidmine. Nimeolemite tuvastamisega tegelesime juba 4. praktikumis - nägime, et eesti keele jaoks on loodud tööriist, mis tuvastab isikunimesid, asukohti ja organisatsioone. Nimeolemeid võib aga käsitleda ka laiemalt - näiteks võib siia alla lugeda ürituste, toodete, haiguste, ravimite vm huvipakkuva tuvastamise.\n",
    "\n",
    "Samaviiteliste üksuste tuvastamise puhul soovime leida tekstist sõnad/väljendid, mis tähistavad sama nähtust või objekti. Näiteks tekstis *\"Galaxy S4 teeb paremaid pilte kui nii mõnigi seebikarp. Telefon maksab küll rohkem, ent tal on ka rohkem funktsioone.\"* tähistavad \"Galaxy S4\" ja \"Telefon\" sama objekti. Samaviiteliste üksuste tuvastamise alamülesanne on asesõnade lahendamine. Üldkasutatavaid tööriistu eesti keelele selle jaoks loodud ei ole.\n",
    "\n",
    "Samuti ei ole üldkasutatavaid tööriistu relatsioonide ehk sõnadevaheliste seoste leidmiseks. \n",
    "\n",
    "Info eraldamiseks vabast tekstist kasutatakse nii reeglipõhiseid meetodeid kui masinõpet. Reeglipõhiste meetodite puhul kasutatakse käsitsi kirjutatud keelelisi mustreid. Need kipuvad olema väga domeenispetsiifilised - õiges valdkonnas võivad anda häid tulemusi, aga ei pruugi olla kergesti kohandatavad teisele valdkonnale. Käsitsi mustrite kirjutamine nõuab loomulikult palju inimtööjõudu, mis on reeglipõhise lähenemise üheks suurimaks miinuseks. Reeglipõhisel lähenemisel võib kasutada erinevaid mustrite kirjutamise viise - regulaaravaldisi või grammatikaid. Regulaaravaldistega olete kõik kokku puutunud, grammatikate kirjutamiseks on EstNLTK-s vahendid olemas, millega täna põgusalt tutvust teeme. Grammatikate eeliseks on võimalus kombineerida erinevates märgenduskihtides olevat infot regulaaravaldiste või ka enda defineeritud märgenduskihtidega. Näiteks, kui meenutate 5. praktikumist nimisõnafraaside eraldamise ülesannet, kus tuli kontrollida järjest asetsevate sõnade sõnaliiki ja vormi, siis seda oleks tegelikult kergem teha grammatika abil.\n",
    "\n",
    "Masinõppepõhised meetodid informatsiooni eraldamisel lähtuvad kas eeldusest, et vaadeldavat ülesannet saab defineerida kui klassifitseerimisprobleemi või on vaja identifitseerida mingit funktsiooni täitvaid tekstisegmente. Kuna masinõppesse me selles aines ei süvene, siis käsitleme ülesannetes ainult reeglipõhiseid meetodeid. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suurte andmetega töötamisest\n",
    "Automaatsel infoeraldusel, samuti aga ka enamikul muudel loomuliku keele automaattöötluse probleemidel, on mõte vaid siis, kui andmeid on palju - rohkem, kui mõistliku ajaga käsitsi läbi jõuaks vaadata. Kui andmeid on piisavalt palju ja/või operatsioonid, mida nendega teha soovime, on piisavalt keerulised, võib aga ka automaattöötlusel ajakulu muutuda kas tüütult või isegi teostamatult suureks. Ilmselt olete pidanud selle aine kodutööde lahendamisel mõnigi kord arvuti taga ootama. Kui ootama peab 20 minutit, on see tüütu, aga kui ooteajaks kujuneb näiteks 2 kuud või lausa 200 aastat? \n",
    "\n",
    "Seetõttu on oluline pöörata tähelepanu oma koodi efektiivsusele. Kuidas aga aru saada, mis koodis täpselt kaua aega võtab ning kui kaua skript veel jookseb? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mis mu koodis kaua aega võtab? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sellele aitab vastust leida *profileerimine*. Pythonis on selleks erinevaid teeke, nt cProfile, line_profiler. Notebookis on eriti mugav kasutada viimast, ehkki see vajab eraldi installimist (conda install line_profiler). Line_profiler'ile saab ette anda oma funktsiooni ning ta näitab iga rea kohta, kui kaua selle täitmiseks aega läks: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Kasutame Notebook magicut line_profileri laadimiseks\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Funktsioon, mida tahame profileerida\n",
    "def dumb_function(my_list, other_list):\n",
    "    other_list2 = set(other_list)\n",
    "    new_thing = []\n",
    "    for word in my_list:\n",
    "        for c in word:\n",
    "            c += 'a'\n",
    "            \n",
    "            # Kontrollime esinemist listis\n",
    "            if 'c' in other_list: \n",
    "                x = 1\n",
    "                \n",
    "            # Kontrollime esinemist setis    \n",
    "            if 'c' in other_list2: \n",
    "                x = 2\n",
    "                \n",
    "            # Kontrollime esinemist setis, \n",
    "            # teisendades listi igal sammul uuesti setiks    \n",
    "            if 'c' in set(other_list): \n",
    "                x = 3\n",
    "                \n",
    "            new_thing.append(c)    \n",
    "    return new_thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Andmed, millel tahame funktsiooni testida\n",
    "# (soovitavalt mingi väiksem alamhulk oma andmetest, mille töötlemiseks ei kulu väga kaua)\n",
    "my_list = ['kana', 'koer', 'vitamiin', 'w4']\n",
    "other_list = ['ka', 'ke', 'ki', 'aa', 'ee', 'oo', 'ta', 'ia', 'ma', 'ra', 'ka', 'ke', 'ki', 'aa', 'ee', 'oo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Jooksutame funktsiooni line_profileriga, et saada iga rea ajakulu\n",
    "%lprun -f dumb_function dumb_function(my_list, other_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nagu näeme, vaadates veerge *Time*, *Per hit* ja *% Time*, läheb siin kiiremini elemendi esinemise kontrollimine setis, seda aga ainult siis, kui listi setiks teisendame ühe korra, mitte igal iteratsioonisammul uuesti."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kui kaua mu kood veel jookseb?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kui oleme koodi tööle pannud ning mingi aja oodanud, kuidas teada, kas töö saab varsti valmis või äkki lähebki 200 aastat, nagu enne sai hirmutatud? Lihtne, aga mitte väga ilus variant on tsüklis iga mingi aja tagant printida välja infot selle kohta, kui kaugele jõudnud oleme. Näiteks kasutades enumerate() funktsiooni:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "6000000\n",
      "7000000\n",
      "8000000\n",
      "9000000\n"
     ]
    }
   ],
   "source": [
    "for idx, number in enumerate(range(10000000)):\n",
    "    if idx%1000000 == 0: # prindib välja ainult iga 1 000 000-nda indeksi\n",
    "        print(idx)\n",
    "    number2 = number + 1 # siin on see, mida tegelikult tsüklis teha tahame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selle lähenemise miinuseks on veel see, et peame teadma, kui kiiresti umbes meie kood jooksma hakkab ehk iga mitme iteratsiooni tagant oleks mõistlik printida. **NB!** Kui lasete välja printida nt kõik 10 000 000 indeksit, siis suure tõenäosusega jookseb Notebook lihtsalt kokku. Seega, et mitte koormata arvuti ressursse liialt printimisega, tuleb selleks valida mõistlik intervall. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sama funktsiooni täitmiseks on aga olemas ka eraldi teek *tqdm* (vaja installida - conda install tqdm), mille kasutamine tsüklis on lihtne, ilus ja mugav:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000000/10000000 [00:05<00:00, 1992403.67it/s]\n"
     ]
    }
   ],
   "source": [
    "for number in tqdm(range(10000000)):\n",
    "    number2 = number +1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mida teha, et mitte ilmaasjata aega raisata?\n",
    "Ühest küljest on oluline **koodi optimeerimine** - mitte teha ühte operatsiooni mitu korda, mitte teha aeganõudvaid operatsioone, kui neid saab asendada kiirematega jms. Vähem oluline pole aga ka **üldine töökorraldus**. \n",
    "\n",
    "Kui on tegu suuremate andmetega, tasub esmalt koodi **testida mingil väiksel alamulgal andmetest** ja vaadata, kas tulemus on selline, nagu loodetud - pole ju mõtet oodata 20 minutit, parandada koodis näpuviga, oodata jälle 20 minutit, parandada järgmine viga jne. \n",
    "\n",
    "Lisaks, kui mingi aeganõudev arvutus on ära tehtud ja tulemus kätte saadud, võib  vahepeal selle **tulemuse salvestada faili** - sel juhul ei pea nt notebooki/arvuti kokkujooksmise, muutujanime kogemata ülekirjutamise vms puhul uuesti otsast peale hakkama. Faili salvestamisel tasub muidugi mõelda sellele, mis kujul salvestada - sobivad näiteks pickle või json, mida on käsitletud eelmistes praktikumides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitteri andmete töötlus\n",
    "\n",
    "Twitter võimaldab üsna lihtsal moel oma säutse korjata. Praktikumi jaoks oleme selle töö juba ära teinud, ent kui on huvi ja tahtmist ka ise säutse korjata, siis juhendi, kuidas seda teha kahel eri viisil, leiate praktikumi kaustast failist **Lisamaterjal_Twitteri_veebiteenused.ipynb**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ülesanne 1. Psüühikahäirete riskigruppi kuuluvate Twitteri-kasutajate tuvastamine (2p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NB!** Tegemist on siiski vaid programmeerimisharjutusega, mille tulemuste põhjal ei tasu kindlasti tegelikult ühtegi Twitteri-kasutajat sildistama hakata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Laias maailmas on tehtud mitmeid uuringuid selle kohta, kuivõrd on võimalik kasutaja sotsiaalmeediapostituste alusel tuvastada seda, kas ta kuulub psüühikahäirete (depressioon, bipolaarne häire, jms) riskigruppi. On leitud mitmeid nii keelelisi kui ka mittekeelelisi tunnuseid, mille abil niisuguseid kasutajaid tuvastada. Siin ülesandes proovime, kas ka eestikeelsete säutsude põhjal midagi sarnast oleks võimalik teha. Selleks on pakitud kaustas \"tweets\" hulk eestikeelseid Twitteri säutse.\n",
    "\n",
    "**NB!** Kuna säutse on palju, siis võib nende töötlemine võtta aega, eriti, kui kasutatav arvuti aeglasepoolne on. Kui tundub, et oma kood on mõistlikult kirjutatud ning näib, et kõigi säutsude töötlemisele kuluks üle 30 minuti (selle ennustuse tegemiseks ei pea 30 min ära ootama - vaadake ülevaltpoolt juhiseid), siis võib kasutada omal valikul ka mingit väiksemat hulka säutsudest.\n",
    "\n",
    "Võtame eeskujuks [selles](http://www.aclweb.org/anthology/W14-3207) artiklis väljatoodud tunnused, mida on kasutanud mitmed autorid ja mis inglise keele peal usaldusväärseid tulemusi on andnud. Need keskenduvad psüühikahäiretega inimestele omastele sümptomitele nagu negatiivsus, vähene sotsiaalne kaasatus jne. Lähtudes eesti keele võimalustest ja vajadustest, kasutame siin ülesandes järgmisi tunnuseid:\n",
    "* **postituse kellaaeg** - psüühikahäiretega inimesi vaevab sageli unetus ja seetõttu postitavad nad tihti öösiti\n",
    "* **teiste kasutajatega suhtlemine** - psüühikahäiretega inimesed on sageli vähem sotsiaalselt kaasatud, Twitteris toimuvad aga n-ö vestlused just teise kasutaja kasutajanime mainimise teel\n",
    "* **1. isiku** sage kasutus - viidatud artiklis on leitud, et psüühikahäiretega inimesed kasutavad ingliskeelsetes postitustes rohkem 1. isiku asesõnu. Kuna eesti keeles võib isikut väljendada ka verbivormis, siis vaatleme nii 1. isikus asesõnu kui vastavaid verbivorme (nii olevikus kui minevikus)\n",
    "* **negatiivsete sõnade** sage kasutus - kasutame EKI emotsioonileksikonis toodud negatiivseid sõnu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seega, püüame leida igale kasutajale nende tunnuste põhjal riskigruppi kuulumise skoori. Kui järjestame kasutajad skooride alusel, võime eeldada, et suurema skoori saanud kasutajad kuuluvad psüühikahäirete riskigruppi suurema tõenäosusega. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Andmed:**\n",
    "* säutsud - kaustas tweets. Kui laadite alla ja pakite kausta lahti, leiate json-formaadis failid\n",
    "* emotsioonileksikon - faili leiab [siit](https://github.com/EKT1/valence/blob/master/valence/sqnad.csv). Kuna kirjanduses ei ole leitud seost positiivsete sõnade ja psüühikahäirete esinemise/mitteesinemise vahel, siis kasutame failist ainult negatiivseid (miinusmärgiga skooriga) sõnu. **NB!** Failis on sõnavormid, mitte lemmad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Skoori arvutamine:**\n",
    "* Arvutame igale kasutajale eraldi skoorid unetuse, negatiivsuse, 1. isiku kasutuse ning teiste kasutajatega suhtlemise kohta. **Lõpuks liidame 4 skoori kokku ning selle alusel järjestame tulemused.**\n",
    "* **Unetus:** iga postitus, mis on tehtud ajavahemikus 01:00 - 06:00 öösel, lisab skoorile ühe punkti. Jagada läbi kasutaja postituste arvuga. (St: kui kasutaja on teinud 20 postitust, millest 5 on tehtud vahemikus 01:00-06:00, siis on kasutaja unetuse skoor 5/20 = 0,4). Vaatame andmetest tunnust 'created_at'\n",
    "* **Sotsiaalne kaasatus:** iga vastus teisele kasutajale lisab skoorile ühe punkti. Jagada läbi kasutaja postituste arvuga. Vaatame andmetest tunnust 'in_reply_to_screen_name'. **NB!** Kuna sotsiaalne kaasatus on pöördtunnus - kõrgem väärtus tähendab väiksemat psüühikahäirete riski - , siis tuleks selle lõppskoorile lisada ette miinusmärk.\n",
    "* **1. isiku kasutus:** iga 1. isikus asesõna või verb lisab skoorile ühe punkti. Jagada läbi kasutaja postituste arvuga. Vaatame andmetest tunnust 'text'\n",
    "* **Negatiivsus:** iga leksikonis negatiivset skoori omav sõna postituses lisab skoorile nii palju punkte nagu emotsioonileksikonis skoorina antud on. Kuna soovime lõpus kõik skoorid kokku liita, tuleks siinkohal ka miinusmärgiga arvude asemel kasutada absoluutväärtusi. Jagada läbi kasutaja postituste arvuga. Vaatame andmetest tunnust 'text'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vaadake peale paari kõige kõrgema ja kõige madalama skoori saanud kasutaja säutsudele. Kas tundub, et metoodikat oleks võimalik tulemuslikult rakendada ka eesti keele peal?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vikipeedia töötlus EstNLTK-s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kui soovime saada palju andmeid erinevate teemade kohta, siis üheks kasulikuks allikaks on kindlasti Vikipeedia. EstNLTK-s on olemas töövahendid eestikeelse Vikipeedia allalaadimiseks ning xml-kujult json-kujule viimiseks (juhised [siin](https://estnltk.github.io/estnltk/1.4.1/tutorials/wikipedia.html#extracting-articles-from-xml-files)). Tuleb aga arvestada, et eestikeelne Vikipeedia on päris suur ja seega kogu Vikipeedia töötlemine võtab kaua aega (\"kaua\" tähendab siinkohal seda, et sõltuvalt arvutist läheb mitmeid tunde või terve päev). Seega kasutame praktikumis juba eelnevalt kogutud ja json-kujule viidud väikest alamhulka Vikipeedia failidest. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ülesanne 2. Isikute ja asukohtadega seotud informatsiooni ekstraheerimine tekstist (2p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oletame, et soovime luua andmebaasi selle kohta, millistes kohtades mingi kunstnik viibis ja milliseid stiile viljeles. Andmebaasi põhjal oleks võimalik lisaks konkreetsete kunstnike andmetele leida infot, missuguses asukohas mingit kunstistiili kõige enam viljeleti. \n",
    "\n",
    "Seega, **eesmärk** on ekstraheerida kunstnike Vikipeedia artiklitest informatsiooni nende:\n",
    "   * A. seotusest erinevate asukohtadega - kes kunstnikest millistes asukohtades (riigid, linnad, külad jne) sündis, elas, töötas, maalis jne.\n",
    "   * B. viljeletud kunstisuundadest\n",
    "   \n",
    "Selle saavutamiseks kasutame EstNLTK vahendeid kombineeritult reeglipõhise lähenemisega."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Andmed:** \n",
    "* Tekstid on pakituna kaustas *kunstnikud*. Kui kausta lahti pakite, leiate ~1500 json-formaadis Vikipeedia artiklit.\n",
    "* Kunstistiilide leksikon on failis *kunstivoolud.txt*. Enamiku kunstivoolude kohta leidub leksikonis enam kui 1 märksõna, mida tuleb väljundis käsitleda võrdsetena (impressionism = impressionist = impressionistlik)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loodava programmi abil:**\n",
    "* loetakse etteantud kaustast sisse failides olevas tekstid,\n",
    "* tuvastatakse tekstidest sellised osalaused, milles sisaldub\n",
    "\n",
    "     * (A) vähemalt üks asukohale viitav nimega üksus (LOC) \n",
    "     \n",
    "         * JA/VÕI\n",
    "     \n",
    "     * (B) vähemalt üks kunstivoolule viitav märksõna (failist *kunstivoolud.txt*)\n",
    "     \n",
    "     \n",
    "* kui tuvastatud osalauses leidub ka pärisnimele viitavaid nimega üksusi (PER), siis tuleks kontrollida, kas vähemalt mõnel neist on midagi ühist failinimes oleva kunstniku nimega (ees- või perekonnanimi - võime oletada, et kui Aado Vabbe failis räägitakse Aadost, siis on tegu ikka selle sama kunstnikuga, aga kui Viiraltist, siis ilmselt mitte). Kui tundub, et osalause räägib ainult mõnest teisest isikust, siis ärme sellest osalausest leitud asukohale ja/või kunstistiilidele tähelepanu pööra.\n",
    "* kui tuvastatud osalauses pärisnimele viitavaid üksusi üldse ei leidu, siis omistame leitud asukohad ja kunstivoolud kunstnikule, kelle nime leiame faili nimest\n",
    "* väljundiks peaks olema kaks json formaadis faili - üks sisaldab iga kunstniku asukohti, teine iga kunstniku viljeldud kunstistiile. Kunstnike nimed on sellisel kujul nagu failinimedes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Millised asukohad on impressionistide seas kõige populaarsemad?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grammatikad EstNLTK-s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eelmises ülesandes käsitlesime info eraldamist väga suletult - leidsime tekstidest eeldefineeritud leksikoni kuuluvaid spetsiifilise temaatikaga märksõnu. Paljud probleemid vajavad aga hoopis üldisemat lähenemist, mille puhul ei saa lähtuda konkreetsetest leksikoniüksustest. Millestki tuleb aga lähtuda ning selleks \"millekski\" võib olla teksti struktuur (vajadusel kombineerituna regulaaravaldiste või leksikonidega). Teksti struktuuri kirjeldamiseks on EstNLTK-s eraldi moodul, mille (natukene puuduliku) dokumentatsiooni leiab [siit](https://estnltk.github.io/estnltk/1.4.1/tutorials/grammar.html). \n",
    "\n",
    "Grammatikate puhul defineerime ära n-ö ehitusplokid, millest meie soovitavad üksused koosnevad, ning mustrid, kuidas plokid omavahel kombineeruvad. Mida see abstraktne jutt tähendab?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impordime kõigepealt grammatikate kirjutamiseks vajaliku:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from estnltk import Text, Regex, Lemmas, Postags, Concatenation, Union, LayerRegex, Intersection, Gaps, Suffix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmas, Postags, Regex, LayerRegex ja Suffix abil saame panna paika ehitusklotsid, Concatenation'i, Union'i, Intersection'i ja Gaps'i abil neid omavahel kombineerida. Kasutame näidiseks järgmist lauset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = Text(\"Eile läksid Mari ja Jüri väiksesse poodi ostma värsket leiba ja vanemat saia.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loome leivatoodete lemmade nimekirja. \n",
    "# Kui tahame pärast leivatooteid Text objekti küljest ka üles leida, tasub kihile panna ka nimi (name)\n",
    "bread = Lemmas('leib', 'sai', name = 'bread')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bread': [{'end': 60, 'start': 55, 'text': 'leiba'},\n",
       "  {'end': 76, 'start': 72, 'text': 'saia'}],\n",
       " 'paragraphs': [{'end': 77, 'start': 0}],\n",
       " 'sentences': [{'end': 77, 'start': 0}],\n",
       " 'text': 'Eile läksid Mari ja Jüri väiksesse poodi ostma värsket leiba ja vanemat saia.',\n",
       " 'words': [{'analysis': [{'clitic': '',\n",
       "     'ending': '0',\n",
       "     'form': '',\n",
       "     'lemma': 'eile',\n",
       "     'partofspeech': 'D',\n",
       "     'root': 'eile',\n",
       "     'root_tokens': ['eile']}],\n",
       "   'end': 4,\n",
       "   'start': 0,\n",
       "   'text': 'Eile'},\n",
       "  {'analysis': [{'clitic': '',\n",
       "     'ending': 'sid',\n",
       "     'form': 'sid',\n",
       "     'lemma': 'minema',\n",
       "     'partofspeech': 'V',\n",
       "     'root': 'mine',\n",
       "     'root_tokens': ['mine']}],\n",
       "   'end': 11,\n",
       "   'start': 5,\n",
       "   'text': 'läksid'},\n",
       "  {'analysis': [{'clitic': '',\n",
       "     'ending': '0',\n",
       "     'form': 'sg n',\n",
       "     'lemma': 'Mari',\n",
       "     'partofspeech': 'H',\n",
       "     'root': 'Mari',\n",
       "     'root_tokens': ['Mari']}],\n",
       "   'end': 16,\n",
       "   'start': 12,\n",
       "   'text': 'Mari'},\n",
       "  {'analysis': [{'clitic': '',\n",
       "     'ending': '0',\n",
       "     'form': '',\n",
       "     'lemma': 'ja',\n",
       "     'partofspeech': 'J',\n",
       "     'root': 'ja',\n",
       "     'root_tokens': ['ja']}],\n",
       "   'end': 19,\n",
       "   'start': 17,\n",
       "   'text': 'ja'},\n",
       "  {'analysis': [{'clitic': '',\n",
       "     'ending': '0',\n",
       "     'form': 'sg n',\n",
       "     'lemma': 'Jüri',\n",
       "     'partofspeech': 'H',\n",
       "     'root': 'Jüri',\n",
       "     'root_tokens': ['Jüri']}],\n",
       "   'end': 24,\n",
       "   'start': 20,\n",
       "   'text': 'Jüri'},\n",
       "  {'analysis': [{'clitic': '',\n",
       "     'ending': 'sse',\n",
       "     'form': 'sg ill',\n",
       "     'lemma': 'väike',\n",
       "     'partofspeech': 'A',\n",
       "     'root': 'väike',\n",
       "     'root_tokens': ['väike']}],\n",
       "   'end': 34,\n",
       "   'start': 25,\n",
       "   'text': 'väiksesse'},\n",
       "  {'analysis': [{'clitic': '',\n",
       "     'ending': '0',\n",
       "     'form': 'adt',\n",
       "     'lemma': 'pood',\n",
       "     'partofspeech': 'S',\n",
       "     'root': 'pood',\n",
       "     'root_tokens': ['pood']}],\n",
       "   'end': 40,\n",
       "   'start': 35,\n",
       "   'text': 'poodi'},\n",
       "  {'analysis': [{'clitic': '',\n",
       "     'ending': 'ma',\n",
       "     'form': 'ma',\n",
       "     'lemma': 'ostma',\n",
       "     'partofspeech': 'V',\n",
       "     'root': 'ost',\n",
       "     'root_tokens': ['ost']}],\n",
       "   'end': 46,\n",
       "   'start': 41,\n",
       "   'text': 'ostma'},\n",
       "  {'analysis': [{'clitic': '',\n",
       "     'ending': 't',\n",
       "     'form': 'sg p',\n",
       "     'lemma': 'värske',\n",
       "     'partofspeech': 'A',\n",
       "     'root': 'värske',\n",
       "     'root_tokens': ['värske']}],\n",
       "   'end': 54,\n",
       "   'start': 47,\n",
       "   'text': 'värsket'},\n",
       "  {'analysis': [{'clitic': '',\n",
       "     'ending': '0',\n",
       "     'form': 'sg p',\n",
       "     'lemma': 'leib',\n",
       "     'partofspeech': 'S',\n",
       "     'root': 'leib',\n",
       "     'root_tokens': ['leib']}],\n",
       "   'end': 60,\n",
       "   'start': 55,\n",
       "   'text': 'leiba'},\n",
       "  {'analysis': [{'clitic': '',\n",
       "     'ending': '0',\n",
       "     'form': '',\n",
       "     'lemma': 'ja',\n",
       "     'partofspeech': 'J',\n",
       "     'root': 'ja',\n",
       "     'root_tokens': ['ja']}],\n",
       "   'end': 63,\n",
       "   'start': 61,\n",
       "   'text': 'ja'},\n",
       "  {'analysis': [{'clitic': '',\n",
       "     'ending': 't',\n",
       "     'form': 'sg p',\n",
       "     'lemma': 'vanem',\n",
       "     'partofspeech': 'C',\n",
       "     'root': 'vanem',\n",
       "     'root_tokens': ['vanem']}],\n",
       "   'end': 71,\n",
       "   'start': 64,\n",
       "   'text': 'vanemat'},\n",
       "  {'analysis': [{'clitic': '',\n",
       "     'ending': '0',\n",
       "     'form': 'sg p',\n",
       "     'lemma': 'sai',\n",
       "     'partofspeech': 'S',\n",
       "     'root': 'sai',\n",
       "     'root_tokens': ['sai']}],\n",
       "   'end': 76,\n",
       "   'start': 72,\n",
       "   'text': 'saia'},\n",
       "  {'analysis': [{'clitic': '',\n",
       "     'ending': '',\n",
       "     'form': '',\n",
       "     'lemma': '.',\n",
       "     'partofspeech': 'Z',\n",
       "     'root': '.',\n",
       "     'root_tokens': ['.']}],\n",
       "   'end': 77,\n",
       "   'start': 76,\n",
       "   'text': '.'}]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bread.annotate(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Näeme, et *annotate* lisab tekstile uue kihi - sedakorda *bread*. Kui me seda teha ei taha, võime lihtsalt küsida vasteid get_matches() meetodi abil:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = Text(\"Eile läksid Mari ja Jüri väiksesse poodi ostma värsket leiba ja vanemat saia.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'end': 60, 'matches': {}, 'name': 'bread', 'start': 55, 'text': 'leiba'},\n",
       " {'end': 76, 'matches': {}, 'name': 'bread', 'start': 72, 'text': 'saia'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bread.get_matches(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et asja huvitavamaks teha, muudame oma grammatika keerulisemaks. Otsime leivatooteid koos omadussõnadega:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defineerime omadussõna\n",
    "adjective = Postags('A', 'C')\n",
    "# Defineerime ka eraldaja, mis sõnade vahel olla võib\n",
    "space = Regex('\\s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Omadussõna võiks paikneda vahetult leivatoote ees. Seega kasutame Concatenation'it nende järjestamiseks:\n",
    "bread_with_adj = Concatenation(adjective, bread, sep = space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'end': 60,\n",
       "  'matches': {'bread': {'end': 60,\n",
       "    'name': 'bread',\n",
       "    'start': 55,\n",
       "    'text': 'leiba'}},\n",
       "  'start': 47,\n",
       "  'text': 'värsket leiba'},\n",
       " {'end': 76,\n",
       "  'matches': {'bread': {'end': 76,\n",
       "    'name': 'bread',\n",
       "    'start': 72,\n",
       "    'text': 'saia'}},\n",
       "  'start': 64,\n",
       "  'text': 'vanemat saia'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bread_with_adj.get_matches(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mida teevad Gaps, Intersection ja Union?\n",
    "* **Gaps** lubab 'klotsidel', mida ta ühendab, paikneda mitte kõrvuti\n",
    "* **Intersection** võimaldab seada mitu tingimust samale 'klotsile'\n",
    "* **Union** on VÕI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Näiteks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lisame oma grammatikasse nimisõna\n",
    "noun = Postags('S')\n",
    "\n",
    "certain_bread = Intersection(noun, bread, name = 'certain_bread')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'end': 60,\n",
       "  'matches': {},\n",
       "  'name': 'certain_bread',\n",
       "  'start': 55,\n",
       "  'text': 'leiba'},\n",
       " {'end': 76,\n",
       "  'matches': {},\n",
       "  'name': 'certain_bread',\n",
       "  'start': 72,\n",
       "  'text': 'saia'}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mõlemad leivatooted saavad praegu nimisõnaanalüüsi, nii et leitavad matchid on samad, mis enne\n",
    "certain_bread.get_matches(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LayerRegex võimaldab defineerida ise kihte ja neid grammatikates kasutada. Defineerime 'ise' praegu nimeüksused:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'named_entities': [{'end': 16, 'label': 'PER', 'start': 12},\n",
       "  {'end': 24, 'label': 'PER', 'start': 20}],\n",
       " 'paragraphs': [{'end': 77, 'start': 0}],\n",
       " 'sentences': [{'end': 77, 'start': 0}],\n",
       " 'text': 'Eile läksid Mari ja Jüri väiksesse poodi ostma värsket leiba ja vanemat saia.',\n",
       " 'words': [{'analysis': [{'clitic': '',\n",
       "     'ending': '0',\n",
       "     'form': '',\n",
       "     'lemma': 'eile',\n",
       "     'partofspeech': 'D',\n",
       "     'root': 'eile',\n",
       "     'root_tokens': ['eile']}],\n",
       "   'end': 4,\n",
       "   'label': 'O',\n",
       "   'start': 0,\n",
       "   'text': 'Eile'},\n",
       "  {'analysis': [{'clitic': '',\n",
       "     'ending': 'sid',\n",
       "     'form': 'sid',\n",
       "     'lemma': 'minema',\n",
       "     'partofspeech': 'V',\n",
       "     'root': 'mine',\n",
       "     'root_tokens': ['mine']}],\n",
       "   'end': 11,\n",
       "   'label': 'O',\n",
       "   'start': 5,\n",
       "   'text': 'läksid'},\n",
       "  {'analysis': [{'clitic': '',\n",
       "     'ending': '0',\n",
       "     'form': 'sg n',\n",
       "     'lemma': 'Mari',\n",
       "     'partofspeech': 'H',\n",
       "     'root': 'Mari',\n",
       "     'root_tokens': ['Mari']}],\n",
       "   'end': 16,\n",
       "   'label': 'B-PER',\n",
       "   'start': 12,\n",
       "   'text': 'Mari'},\n",
       "  {'analysis': [{'clitic': '',\n",
       "     'ending': '0',\n",
       "     'form': '',\n",
       "     'lemma': 'ja',\n",
       "     'partofspeech': 'J',\n",
       "     'root': 'ja',\n",
       "     'root_tokens': ['ja']}],\n",
       "   'end': 19,\n",
       "   'label': 'O',\n",
       "   'start': 17,\n",
       "   'text': 'ja'},\n",
       "  {'analysis': [{'clitic': '',\n",
       "     'ending': '0',\n",
       "     'form': 'sg n',\n",
       "     'lemma': 'Jüri',\n",
       "     'partofspeech': 'H',\n",
       "     'root': 'Jüri',\n",
       "     'root_tokens': ['Jüri']}],\n",
       "   'end': 24,\n",
       "   'label': 'B-PER',\n",
       "   'start': 20,\n",
       "   'text': 'Jüri'},\n",
       "  {'analysis': [{'clitic': '',\n",
       "     'ending': 'sse',\n",
       "     'form': 'sg ill',\n",
       "     'lemma': 'väike',\n",
       "     'partofspeech': 'A',\n",
       "     'root': 'väike',\n",
       "     'root_tokens': ['väike']}],\n",
       "   'end': 34,\n",
       "   'label': 'O',\n",
       "   'start': 25,\n",
       "   'text': 'väiksesse'},\n",
       "  {'analysis': [{'clitic': '',\n",
       "     'ending': '0',\n",
       "     'form': 'adt',\n",
       "     'lemma': 'pood',\n",
       "     'partofspeech': 'S',\n",
       "     'root': 'pood',\n",
       "     'root_tokens': ['pood']}],\n",
       "   'end': 40,\n",
       "   'label': 'O',\n",
       "   'start': 35,\n",
       "   'text': 'poodi'},\n",
       "  {'analysis': [{'clitic': '',\n",
       "     'ending': 'ma',\n",
       "     'form': 'ma',\n",
       "     'lemma': 'ostma',\n",
       "     'partofspeech': 'V',\n",
       "     'root': 'ost',\n",
       "     'root_tokens': ['ost']}],\n",
       "   'end': 46,\n",
       "   'label': 'O',\n",
       "   'start': 41,\n",
       "   'text': 'ostma'},\n",
       "  {'analysis': [{'clitic': '',\n",
       "     'ending': 't',\n",
       "     'form': 'sg p',\n",
       "     'lemma': 'värske',\n",
       "     'partofspeech': 'A',\n",
       "     'root': 'värske',\n",
       "     'root_tokens': ['värske']}],\n",
       "   'end': 54,\n",
       "   'label': 'O',\n",
       "   'start': 47,\n",
       "   'text': 'värsket'},\n",
       "  {'analysis': [{'clitic': '',\n",
       "     'ending': '0',\n",
       "     'form': 'sg p',\n",
       "     'lemma': 'leib',\n",
       "     'partofspeech': 'S',\n",
       "     'root': 'leib',\n",
       "     'root_tokens': ['leib']}],\n",
       "   'end': 60,\n",
       "   'label': 'O',\n",
       "   'start': 55,\n",
       "   'text': 'leiba'},\n",
       "  {'analysis': [{'clitic': '',\n",
       "     'ending': '0',\n",
       "     'form': '',\n",
       "     'lemma': 'ja',\n",
       "     'partofspeech': 'J',\n",
       "     'root': 'ja',\n",
       "     'root_tokens': ['ja']}],\n",
       "   'end': 63,\n",
       "   'label': 'O',\n",
       "   'start': 61,\n",
       "   'text': 'ja'},\n",
       "  {'analysis': [{'clitic': '',\n",
       "     'ending': 't',\n",
       "     'form': 'sg p',\n",
       "     'lemma': 'vanem',\n",
       "     'partofspeech': 'C',\n",
       "     'root': 'vanem',\n",
       "     'root_tokens': ['vanem']}],\n",
       "   'end': 71,\n",
       "   'label': 'O',\n",
       "   'start': 64,\n",
       "   'text': 'vanemat'},\n",
       "  {'analysis': [{'clitic': '',\n",
       "     'ending': '0',\n",
       "     'form': 'sg p',\n",
       "     'lemma': 'sai',\n",
       "     'partofspeech': 'S',\n",
       "     'root': 'sai',\n",
       "     'root_tokens': ['sai']}],\n",
       "   'end': 76,\n",
       "   'label': 'O',\n",
       "   'start': 72,\n",
       "   'text': 'saia'},\n",
       "  {'analysis': [{'clitic': '',\n",
       "     'ending': '',\n",
       "     'form': '',\n",
       "     'lemma': '.',\n",
       "     'partofspeech': 'Z',\n",
       "     'root': '.',\n",
       "     'root_tokens': ['.']}],\n",
       "   'end': 77,\n",
       "   'label': 'O',\n",
       "   'start': 76,\n",
       "   'text': '.'}]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.tag_named_entities()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Leiame M-tähega algavad nimed\n",
    "m_names = LayerRegex('named_entities', 'M.*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'end': 16, 'matches': {}, 'start': 12, 'text': 'Mari'}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_names.get_matches(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ülesanne 3. Katsetame grammatika kirjutamist (1p + 0,5 boonuspunkti)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Püüdke kirjutada grammatika, mis leiab tekstist n-ö laiemas tähenduses nimisõnafraase: erinevalt keeleteadusest, ei ole info eraldamise seisukohast üldiselt vahet, kas tekstis on öeldud \"poes on hea teenindus\" või \"teenindus oli poes hea\" või hoopis \"poes teenindus hea\". Katsetage grammatikat faili *hinnavaatlus.csv* tekstide peal. Grammatika peaks katma minimaalselt järgmised nädisstruktuurid:\n",
    "* suur õun\n",
    "* suur ja sinine õun\n",
    "* suur sinine õun\n",
    "* õun on suur\n",
    "* õun suur\n",
    "* õun on suur ja sinine\n",
    "\n",
    "Seejuures sõnad 'õun', 'suur' ja 'sinine' peaksid olema üldistatud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leidke oma grammatikaga *hinnavaatlus.csv* failist vastavad fraasid ning viige need kujule \"suur õun\" (omadussõna, millele järgneb nimisõna). Kui leidub mitu omadussõna, tehke igast eraldi fraas (\"suur ja sinine õun\" -> \"suur õun\", \"sinine õun\"). Sõnad võivad jääda sellisesse vormi, nagu nad on. (Aga kui leiate indeksite põhjal lemmad ja moodustate fraasid lemmadest, teenite **0,5 boonuspunkti**.)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
